{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SERIEMA: A Framework to Enhance Clustering Stability by Fusing Multimodal Data\n",
    " \n",
    "A novel multimodal framework that seamlessly integrates categorical, numerical, and text data to bolster clustering robustness. It represents a novel approach to customer segmentation and paves the way for future exploration of data fusion techniques in the context of marketing and other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dependencies and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from statistics import mean, stdev\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from config.definitions import ROOT_DIR\n",
    "\n",
    "os.chdir(ROOT_DIR + '\\\\src\\\\model\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    HfArgumentParser,\n",
    "    set_seed,\n",
    "    TrainerCallback,\n",
    "    EarlyStoppingCallback,\n",
    "    Trainer,\n",
    "    EvalPrediction,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "from multimodal_exp_args import (\n",
    "    MultimodalDataTrainingArguments,\n",
    "    ModelArguments,\n",
    "    OurTrainingArguments,\n",
    "    ComputerStabilityArguments,\n",
    ")\n",
    "\n",
    "from evaluation import calc_stability_metrics\n",
    "from data import load_data_from_folder, load_data_into_folds\n",
    "from multimodal import TabularConfig\n",
    "from multimodal import AutoModelWithTabular\n",
    "from multimodal import CustomTrainer\n",
    "from util import create_dir_if_not_exists, get_args_info_as_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Data, Training, and Stability arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser(\n",
    "    (ModelArguments, MultimodalDataTrainingArguments, OurTrainingArguments, ComputerStabilityArguments)\n",
    ")\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path='bert-base-multilingual-uncased',\n",
    "    config_name=None, \n",
    "    tokenizer_name='bert-base-multilingual-uncased', \n",
    "    cache_dir=None\n",
    "    )\n",
    "\n",
    "data_args = MultimodalDataTrainingArguments(\n",
    "    data_path=ROOT_DIR + '\\\\src\\\\model\\\\notebook\\\\', \n",
    "    create_folds=False, num_folds=5, \n",
    "    validation_ratio=0.2, \n",
    "    freeze_transformer_weights=True, \n",
    "    latent_dim=3, bn_enc=False, \n",
    "    bn_dec=False, vae_out_dim=1, \n",
    "    VAE_architecture=[1000, 1000, 600, 300], \n",
    "    column_info_path='column_info.json', \n",
    "    column_info={'text_cols': ['text'], 'cat_cols': [], 'num_cols': ['review_count', 'useful_user', 'funny_user', 'cool_user', 'fans', 'average_stars', 'compliment_hot', 'compliment_more', 'compliment_profile', 'compliment_cute', 'compliment_list', 'compliment_note', 'compliment_plain', 'compliment_cool', 'compliment_funny', 'compliment_writer', 'compliment_photos', 'friend_count', 'elite_count', 'yelp_since_YRMO', 'yelp_since_year', 'stars', 'useful_review', 'funny_review', 'cool_review'], 'text_col_sep_token': None}, \n",
    "    categorical_encode_type='binary', \n",
    "    numerical_transformer_method='none',\n",
    "    mlp_division=4, \n",
    "    vae_division=2, \n",
    "    combine_feat_method='gating_on_cat_and_num_feats_then_sum',\n",
    "    mlp_dropout=0.1, \n",
    "    vae_dropout=0, \n",
    "    numerical_bn=True, \n",
    "    use_simple_classifier=False, \n",
    "    mlp_act='relu', \n",
    "    vae_act='lrelu', \n",
    "    gating_beta=0.2, \n",
    "    train_csv_name='train.csv',\n",
    "    val_csv_name='val.csv', \n",
    "    test_csv_name='test.csv', \n",
    "    train_samples=20, \n",
    "    val_samples=20, \n",
    "    test_samples=20\n",
    "    )\n",
    "\n",
    "training_args = OurTrainingArguments(\n",
    "    output_dir=ROOT_DIR + '\\\\src\\\\model\\\\notebook\\\\', \n",
    "    overwrite_output_dir=True, \n",
    "    do_train=True, \n",
    "    do_eval=True,\n",
    "    do_predict=True, \n",
    "    evaluation_strategy='steps', \n",
    "    prediction_loss_only=False,\n",
    "    per_device_train_batch_size=10, \n",
    "    per_device_eval_batch_size=10,\n",
    "    per_gpu_train_batch_size=None,\n",
    "    per_gpu_eval_batch_size=None,\n",
    "    gradient_accumulation_steps=1, \n",
    "    eval_accumulation_steps=None, \n",
    "    eval_delay=0, \n",
    "    eval_steps=5,\n",
    "    learning_rate=0.003, \n",
    "    weight_decay=0.0, \n",
    "    adam_beta1=0.9, \n",
    "    adam_beta2=0.999, \n",
    "    adam_epsilon=1e-08, \n",
    "    max_grad_norm=1.0, \n",
    "    num_train_epochs=5, \n",
    "    max_steps=-1, \n",
    "    lr_scheduler_type='linear', \n",
    "    warmup_ratio=0.0, \n",
    "    warmup_steps=0, \n",
    "    log_level='passive',\n",
    "    log_level_replica='passive',\n",
    "    log_on_each_node=True, \n",
    "    logging_dir=ROOT_DIR + '\\\\src\\\\model\\\\notebook\\\\', \n",
    "    logging_strategy='steps', \n",
    "    logging_first_step=False, \n",
    "    logging_steps=5, \n",
    "    logging_nan_inf_filter=True, \n",
    "    save_strategy='steps', \n",
    "    save_steps=3000, \n",
    "    save_total_limit=None, \n",
    "    save_on_each_node=False, \n",
    "    no_cuda=False,\n",
    "    use_mps_device=False, \n",
    "    seed=42, data_seed=None,\n",
    "    jit_mode_eval=False, \n",
    "    use_ipex=False,\n",
    "    bf16=False, \n",
    "    fp16=False, \n",
    "    fp16_opt_level='O1', \n",
    "    half_precision_backend='auto', \n",
    "    bf16_full_eval=False, \n",
    "    fp16_full_eval=False,\n",
    "    tf32=None, local_rank=-1,\n",
    "    experiment_name='SERIEMA - Notebook'\n",
    ")\n",
    "\n",
    "stability_args = ComputerStabilityArguments(\n",
    "    clusters=[2], \n",
    "    k_means_random_state=42, \n",
    "    n_samples=50, \n",
    "    randomStateSeed=1, \n",
    "    compute_stability_steps=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2024 16:50:21 - INFO - data.data_utils -   0 categorical columns\n",
      "01/24/2024 16:50:21 - INFO - data.data_utils -   25 numerical columns\n",
      "01/24/2024 16:50:21 - INFO - data.load_data -   Text columns: ['text']\n",
      "01/24/2024 16:50:21 - INFO - data.load_data -   Raw text example: Cutest spot we've visited so far. The ambiance is great, very friendly service and the food was wonderful. We had the Eggs Benedict and Spicy Boudin Omelette, both were great choices. The peach and strawberry mimosas were a hit as well. I definitely recommend it!\n",
      "01/24/2024 16:50:21 - INFO - data.data_utils -   0 categorical columns\n",
      "01/24/2024 16:50:21 - INFO - data.data_utils -   25 numerical columns\n",
      "01/24/2024 16:50:21 - INFO - data.load_data -   Text columns: ['text']\n",
      "01/24/2024 16:50:21 - INFO - data.load_data -   Raw text example: I've been here maybe a dozen times?  It's a pretty gothy-feeling bar, that's for sure!  Very dimly lit maroon-colored with random Russian-esque art haphazardly hung about and small cutesy little courtyard in the back. \n",
      "You can get your absinthe fix here (although yeck, the stuff is foul!) or just stick to your regular spirits and beer.  I think they have all sorts of vodka here - that's their shtick?  That and their absinthe.  I vodka, hate absinthe - do you get what I'm trying to say here?\n",
      "\n",
      "Anyway, the bartenders are typically quite friendly, the drinks really vary in alcohol potency, but I stick to the basics here.  I don't think they have specialty cocktails to the best of my knowledge.\n",
      "\n",
      "I love sitting up against to the booth seating to the left and just observing the people ebbing and flowing from Pravda - usually younger hipster kids, pseudo-goth people, and sometimes older people set up shop here with their books and laptops (there are classic books available on the shelves here, should you feel a moment of bold literacy). \n",
      "\n",
      "There's free wifi here, which is good as my phone signal usually blows in here.  \n",
      "\n",
      "I would say that it's never gotten REALLY crowded in here, so it's a perfect pre-gaming or post-gaming site.\n",
      "01/24/2024 16:50:21 - INFO - data.data_utils -   0 categorical columns\n",
      "01/24/2024 16:50:21 - INFO - data.data_utils -   25 numerical columns\n",
      "01/24/2024 16:50:21 - INFO - data.load_data -   Text columns: ['text']\n",
      "01/24/2024 16:50:21 - INFO - data.load_data -   Raw text example: We went here for a friends 50th birthday party on a Saturday night. I don't remember ever being here before however my wife said we had, and for those of you that are married you know it is a good idea to agree with your wife.\n",
      "\n",
      "We arrived a little early so we decided to grab a couple of appetizers and some drinks while we waited. We ordered oysters and nachos. They also had a happy hour special that went in till 7 p.m. which was buy one get one for limited drink items.\n",
      "\n",
      "All the servers were very nice, including the folks that greet you when you first walk in. They were friendly, helpful, and delivered our orders timely and accurately. The prices here are not cheap, they are not super expensive, I would categorize them as somewhere in the middle.\n",
      "\n",
      "Oysters were little on the small side. I'm not sure if that's quality issue or a time of year issue. The nachos were a big portion for an appetizer, and were loaded with lots of good stuff including pork. We seriously enjoyed digging into these nachos. The drinks seemed a little light on the alcohol or maybe just a little heavy on the mixers. We hard margaritas and they were a little sweet for my taste but still okay.\n",
      "\n",
      "The party we are attending was outside on the patio. The patio is nice and it has ample seating for a decent sized group.  The service out here with a little bit slower but the place was pretty busy so that makes sense. They also had live music which consisted of one man and a guitar. My overall experience at this place was a good one and I would certainly go to this place again if I found myself in the area.\n"
     ]
    }
   ],
   "source": [
    "create_dir_if_not_exists(training_args.output_dir)\n",
    "stream_handler = logging.StreamHandler(sys.stderr)\n",
    "file_handler = logging.FileHandler(\n",
    "    filename=os.path.join(training_args.output_dir, \"train_log.txt\"),  encoding='utf-8', mode=\"w+\"\n",
    ")\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    level=logging.INFO if training_args.local_rank in [-1, 0] else logging.WARN,\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[stream_handler, file_handler],\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name\n",
    "    if model_args.tokenizer_name\n",
    "    else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    ")\n",
    "\n",
    "if not data_args.create_folds:\n",
    "    train_dataset, val_dataset, test_dataset = load_data_from_folder(\n",
    "        data_args.train_csv_name, \n",
    "        data_args.val_csv_name, \n",
    "        data_args.test_csv_name,\n",
    "        data_args.train_samples,\n",
    "        data_args.val_samples,\n",
    "        data_args.test_samples,\n",
    "        data_args.data_path,\n",
    "        data_args.column_info[\"text_cols\"],\n",
    "        tokenizer,\n",
    "        categorical_cols=data_args.column_info[\"cat_cols\"],\n",
    "        numerical_cols=data_args.column_info[\"num_cols\"],\n",
    "        categorical_encode_type=data_args.categorical_encode_type,\n",
    "        numerical_transformer_method=data_args.numerical_transformer_method,\n",
    "        sep_text_token_str=tokenizer.sep_token\n",
    "        if not data_args.column_info[\"text_col_sep_token\"]\n",
    "        else data_args.column_info[\"text_col_sep_token\"],\n",
    "        max_token_length=training_args.max_token_length,\n",
    "        debug=training_args.debug_dataset,\n",
    "        debug_dataset_size=training_args.debug_dataset_size,\n",
    "    )\n",
    "    train_datasets = [train_dataset]\n",
    "    val_datasets = [val_dataset]\n",
    "    test_datasets = [test_dataset]\n",
    "else:\n",
    "    train_datasets, val_datasets, test_datasets = load_data_into_folds(\n",
    "        data_args.data_path,\n",
    "        data_args.num_folds,\n",
    "        data_args.validation_ratio,\n",
    "        data_args.column_info[\"text_cols\"],\n",
    "        tokenizer,\n",
    "        categorical_cols=data_args.column_info[\"cat_cols\"],\n",
    "        numerical_cols=data_args.column_info[\"num_cols\"],\n",
    "        categorical_encode_type=data_args.categorical_encode_type,\n",
    "        numerical_transformer_method=data_args.numerical_transformer_method,\n",
    "        sep_text_token_str=tokenizer.sep_token\n",
    "        if not data_args.column_info[\"text_col_sep_token\"]\n",
    "        else data_args.column_info[\"text_col_sep_token\"],\n",
    "        max_token_length=training_args.max_token_length,\n",
    "        debug=training_args.debug_dataset,\n",
    "        debug_dataset_size=training_args.debug_dataset_size,\n",
    "    )\n",
    "train_dataset = train_datasets[0]\n",
    "set_seed(training_args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertWithTabular: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertWithTabular from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertWithTabular from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertWithTabular were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['bert.tabular_classifier.decoder.layers.0.weight', 'bert.tabular_classifier.decoder.bn.0.bias', 'bert.tabular_combiner.layer_norm.bias', 'bert.tabular_classifier.decoder.bn.2.num_batches_tracked', 'bert.tabular_combiner.num_bn.bias', 'bert.tabular_combiner.num_bn.num_batches_tracked', 'bert.tabular_combiner.g_num_layer.weight', 'bert.tabular_classifier.decoder.bn.0.running_var', 'bert.tabular_classifier.decoder.bn.1.running_var', 'bert.tabular_classifier.encoder.layers.4.weight', 'bert.tabular_combiner.g_num_layer.bias', 'bert.tabular_classifier.pi', 'bert.tabular_classifier.decoder.bn.2.bias', 'bert.tabular_classifier.encoder.layers.3.bias', 'bert.tabular_classifier.encoder.bn.3.weight', 'bert.tabular_classifier.encoder.layers.2.bias', 'bert.tabular_classifier.encoder.bn.0.running_var', 'bert.tabular_classifier.decoder.layers.1.bias', 'bert.tabular_classifier.encoder.bn.1.running_var', 'bert.tabular_classifier.encoder.bn.1.num_batches_tracked', 'bert.tabular_classifier.decoder.bn.3.bias', 'bert.tabular_classifier.encoder.bn.0.weight', 'bert.tabular_classifier.decoder.bn.0.running_mean', 'bert.tabular_classifier.encoder.layers.0.bias', 'bert.tabular_combiner.h_num_layer.weight', 'bert.tabular_classifier.encoder.bn.1.weight', 'bert.tabular_classifier.decoder.layers.1.weight', 'bert.tabular_classifier.encoder.layers.0.weight', 'bert.tabular_classifier.encoder.bn.0.bias', 'bert.tabular_classifier.decoder.bn.3.weight', 'bert.tabular_classifier.encoder.bn.3.num_batches_tracked', 'bert.tabular_classifier.encoder.layers.1.bias', 'bert.tabular_classifier.encoder.bn.3.running_mean', 'bert.tabular_combiner.num_bn.running_var', 'bert.tabular_classifier.decoder.bn.3.num_batches_tracked', 'bert.tabular_classifier.decoder.bn.2.weight', 'bert.tabular_classifier.decoder.bn.1.running_mean', 'bert.tabular_classifier.encoder.layers.4.bias', 'bert.tabular_classifier.encoder.layers.3.weight', 'bert.tabular_classifier.decoder.bn.3.running_var', 'bert.tabular_classifier.decoder.bn.2.running_var', 'bert.tabular_classifier.encoder.layers.2.weight', 'bert.tabular_classifier.logvar', 'bert.tabular_classifier.decoder.bn.0.weight', 'bert.tabular_classifier.decoder.layers.2.bias', 'bert.tabular_classifier.mu', 'bert.tabular_combiner.num_bn.running_mean', 'bert.tabular_classifier.encoder.layers.5.bias', 'bert.tabular_classifier.decoder.bn.1.num_batches_tracked', 'bert.tabular_classifier.encoder.bn.2.running_mean', 'bert.tabular_classifier.encoder.layers.1.weight', 'bert.tabular_classifier.decoder.layers.2.weight', 'bert.tabular_classifier.encoder.bn.2.num_batches_tracked', 'bert.tabular_classifier.decoder.bn.1.weight', 'bert.tabular_combiner.layer_norm.weight', 'bert.tabular_classifier.decoder.layers.3.bias', 'bert.tabular_classifier.encoder.bn.1.bias', 'bert.tabular_classifier.encoder.bn.2.bias', 'bert.tabular_classifier.encoder.bn.2.weight', 'bert.tabular_classifier.encoder.bn.0.running_mean', 'bert.tabular_classifier.encoder.bn.0.num_batches_tracked', 'bert.tabular_classifier.decoder.bn.2.running_mean', 'bert.tabular_classifier.encoder.bn.3.running_var', 'bert.tabular_classifier.decoder.bn.3.running_mean', 'bert.tabular_classifier.encoder.bn.2.running_var', 'bert.tabular_classifier.encoder.bn.1.running_mean', 'bert.tabular_classifier.decoder.layers.0.bias', 'bert.tabular_classifier.encoder.bn.3.bias', 'bert.tabular_classifier.decoder.bn.0.num_batches_tracked', 'bert.tabular_classifier.decoder.bn.1.bias', 'bert.tabular_classifier.encoder.layers.5.weight', 'bert.tabular_classifier.decoder.layers.3.weight', 'bert.tabular_combiner.num_bn.weight', 'bert.tabular_combiner.h_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "total_results = []\n",
    "for i, (train_dataset, val_dataset, test_dataset) in enumerate(\n",
    "    zip(train_datasets, val_datasets, test_datasets)\n",
    "):\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_args.config_name\n",
    "        if model_args.config_name\n",
    "        else model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )\n",
    "    tabular_config = TabularConfig(\n",
    "        cat_feat_dim=train_dataset.cat_feats.shape[1]\n",
    "        if train_dataset.cat_feats is not None\n",
    "        else 0,\n",
    "        numerical_feat_dim=train_dataset.numerical_feats.shape[1]\n",
    "        if train_dataset.numerical_feats is not None\n",
    "        else 0,\n",
    "        **vars(data_args),\n",
    "    )\n",
    "    config.tabular_config = tabular_config\n",
    "    model = AutoModelWithTabular.from_pretrained(\n",
    "        model_args.config_name\n",
    "        if model_args.config_name\n",
    "        else model_args.model_name_or_path,\n",
    "        config=config,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Callback based on TrainerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2024 16:50:23 - INFO - multimodal_exp_args -   PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "class CustomCallback(TrainerCallback):\n",
    "    def __init__(self, trainer, stability_args):\n",
    "        super().__init__()\n",
    "        self.trainer = trainer\n",
    "        self.stability_args = stability_args\n",
    "\n",
    "    def calc_stability(self, coming_from_train_end = False):\n",
    "        if (self.stability_args.compute_stability_steps > 0 \n",
    "            and self.state.global_step % self.stability_args.compute_stability_steps == 0) or coming_from_train_end:\n",
    "\n",
    "            val_outputs = self.trainer.predict(self.trainer.eval_dataset).predictions\n",
    "            stability_out = calc_stability_metrics(\n",
    "                val_outputs, \n",
    "                self.stability_args.clusters, \n",
    "                self.stability_args.k_means_random_state,\n",
    "                self.stability_args.n_samples, \n",
    "                self.stability_args.randomStateSeed\n",
    "            )\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        self.calc_stability()\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        self.calc_stability(coming_from_train_end = True)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        train_dataset = train_dataset,\n",
    "        eval_dataset = val_dataset,\n",
    "        compute_metrics = None, #evaluation strategy to adopt during training calling by evaluation_strategy. See bertvaewithtabular/multimodal_transformers/model/custom_trainer.py#L261\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "trainer.add_callback(CustomCallback(trainer, stability_args))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\SIRIEMA\\lib\\site-packages\\transformers\\trainer.py:1504: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\SIRIEMA\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 10\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 10\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 173284368\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25870c80dfde4416aeb453448d2b2be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2024 16:52:05 - INFO - multimodal.custom_trainer -   ***** Running Evaluation *****\n",
      "01/24/2024 16:52:05 - INFO - multimodal.custom_trainer -     Num examples = 20\n",
      "01/24/2024 16:52:05 - INFO - multimodal.custom_trainer -     Batch size = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10391.2266, 'learning_rate': 0.0015, 'epoch': 2.5}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e5ba8ced76449fba9413e8d703bedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3821.28369140625, 'eval_runtime': 24.6076, 'eval_samples_per_second': 0.813, 'eval_steps_per_second': 0.081, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2024 16:54:02 - INFO - multimodal.custom_trainer -   ***** Running Evaluation *****\n",
      "01/24/2024 16:54:02 - INFO - multimodal.custom_trainer -     Num examples = 20\n",
      "01/24/2024 16:54:02 - INFO - multimodal.custom_trainer -     Batch size = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3467.8762, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f90a6338d14923b0c60f015c879933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "01/24/2024 16:54:25 - INFO - multimodal.custom_trainer -   ***** Running Prediction *****\n",
      "01/24/2024 16:54:25 - INFO - multimodal.custom_trainer -     Num examples = 20\n",
      "01/24/2024 16:54:25 - INFO - multimodal.custom_trainer -     Batch size = 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1482.9000244140625, 'eval_runtime': 23.0298, 'eval_samples_per_second': 0.868, 'eval_steps_per_second': 0.087, 'epoch': 5.0}\n",
      "{'train_runtime': 241.3256, 'train_samples_per_second': 0.414, 'train_steps_per_second': 0.041, 'train_loss': 6929.5513671875, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79c5d0f7bff4f8087a0fb205a424d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2024 16:54:50 - INFO - evaluation -   Computing adjusted_rand_score k = 2\n",
      "01/24/2024 16:54:51 - INFO - evaluation -   Computing adjusted_mutual_info_score k = 2\n",
      "01/24/2024 16:54:53 - INFO - evaluation -   Computing [bagclust]\n",
      "01/24/2024 16:54:54 - INFO - evaluation -   Computing [han]\n",
      "01/24/2024 16:54:57 - INFO - evaluation -   Computing [OTstab]\n",
      "01/24/2024 16:54:57 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: sigma summary: Min. : 33554432 |1st Qu. : 33554432 |Median : 33554432 |Mean : 33554432 |3rd Qu. : 33554432 |Max. : 33554432 |\n",
      "\n",
      "01/24/2024 16:54:57 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #100 error is: 12.7745628466278\n",
      "\n",
      "01/24/2024 16:54:57 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #200 error is: 0.858761393936545\n",
      "\n",
      "01/24/2024 16:54:58 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #300 error is: 0.560467654368274\n",
      "\n",
      "01/24/2024 16:54:58 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #400 error is: 0.422080194147811\n",
      "\n",
      "01/24/2024 16:54:58 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #500 error is: 0.413656634020063\n",
      "\n",
      "01/24/2024 16:54:58 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #600 error is: 0.411694900427502\n",
      "\n",
      "01/24/2024 16:54:58 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #700 error is: 0.410428112456663\n",
      "\n",
      "01/24/2024 16:54:58 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #800 error is: 0.409889141825525\n",
      "\n",
      "01/24/2024 16:54:58 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #900 error is: 0.409635459929036\n",
      "\n",
      "01/24/2024 16:54:58 - WARNING - rpy2.rinterface_lib.callbacks -   R[write to console]: Epoch: Iteration #1000 error is: 0.409444168298892\n",
      "\n",
      "Saving model checkpoint to C:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\SIRIEMA\\src\\model\\notebook\\\n",
      "Configuration saved in C:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\SIRIEMA\\src\\model\\notebook\\config.json\n",
      "Model weights saved in C:\\Users\\FS-Ma\\OneDrive\\Documents\\projects\\SIRIEMA\\src\\model\\notebook\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train(\n",
    "    model_path=model_args.model_name_or_path\n",
    "    if os.path.isdir(model_args.model_name_or_path)\n",
    "    else None\n",
    ")\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on validation and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2024 16:54:59 - INFO - multimodal.custom_trainer -   ***** Running Evaluation *****\n",
      "01/24/2024 16:54:59 - INFO - multimodal.custom_trainer -     Num examples = 20\n",
      "01/24/2024 16:54:59 - INFO - multimodal.custom_trainer -     Batch size = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111afc65a77e48ed99eb5ee234e835f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2024 16:55:30 - INFO - multimodal.custom_trainer -   ***** Running Prediction *****\n",
      "01/24/2024 16:55:30 - INFO - multimodal.custom_trainer -     Num examples = 20\n",
      "01/24/2024 16:55:30 - INFO - multimodal.custom_trainer -     Batch size = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe93c5bc519241d4b37651036976ef74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_result = trainer.evaluate(eval_dataset=val_dataset)\n",
    "output_eval_file = os.path.join(\n",
    "    training_args.output_dir, f\"eval_metric_results.txt\"\n",
    ")\n",
    "if trainer.is_world_process_zero():\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        for key, value in eval_result.items():\n",
    "            writer.write(\"%s = %s\\n\" % (key, value))            \n",
    "\n",
    "output_predict_eval = os.path.join(\n",
    "    training_args.output_dir, f\"predict_val.csv\"\n",
    ")\n",
    "predictions = trainer.predict(test_dataset=val_dataset).predictions\n",
    "pd.DataFrame(predictions).to_csv(output_predict_eval, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "01/24/2024 16:56:04 - INFO - multimodal.custom_trainer -   ***** Running Prediction *****\n",
      "01/24/2024 16:56:04 - INFO - multimodal.custom_trainer -     Num examples = 20\n",
      "01/24/2024 16:56:04 - INFO - multimodal.custom_trainer -     Batch size = 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb7b2fe6e4d40dfb3dd6b41f45aae1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_dataset=test_dataset).predictions\n",
    "output_test_file = os.path.join(\n",
    "    training_args.output_dir, f\"predict_test.csv\"\n",
    ")\n",
    "if trainer.is_world_process_zero():\n",
    "    pd.DataFrame(predictions).to_csv(output_test_file, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customer-segmentation-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
